alertmanager:

  ## Configure pod disruption budgets for Alertmanager
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  ## https://github.com/kubernetes/kubernetes/issues/45398
  # данная настройка влияет на то, сколько Pod может быть одновременно выключено (распространяется только на eviction API). 
  # Она позволяет гарантировать, что при обслуживании кластера Kubernetes не будут выключены все Pod'ы с приложением.
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
    maxUnavailable: ""

  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ## см настройки для slack в /home/baggurd/Dropbox/Projects/nginx_learning/kubernetes/prometheus_operator/operator_values.yaml
  ## см. также здесь /home/baggurd/Dropbox/Projects/nginx_learning/ansible/roles/alertmanager/templates/alertmanager.yml
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - receiver: 'null'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog"
    receivers:
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'


  ingress:
    enabled: true

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    ingressClassName: nginx

    annotations:
      nginx.ingress.kubernetes.io/auth-type: basic
      # Секрет должен быть создан заранее с именем admin-basic-auth'
      nginx.ingress.kubernetes.io/auth-secret: admin-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
      kubernetes.io/tls-acme: "true"
      acme.cert-manager.io/http01-edit-in-place: "true"

    labels: {}

    ## Override ingress to a different defined port on the service
    # servicePort: 8081
    ## Override ingress to a different service then the default, this is useful if you need to
    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)
    # serviceName: kube-prometheus-stack-alertmanager-0

    ## Hosts must be provided if Ingress is enabled.
    ##
    hosts:
      - alertmanager.kubernetes.basov.world
      # - alertmanager.domain.com

    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
    ##
    paths:
      - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    pathType: Prefix

    ## TLS configuration for Alertmanager Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
      - secretName: alertmanager-general-tls
        hosts:
          - alertmanager.kubernetes.basov.world
    # - secretName: alertmanager-general-tls
    #   hosts:
    #   - alertmanager.example.com


  ## Settings affecting alertmanagerSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
  ##
  alertmanagerSpec:

    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1

    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    ## Здесь мы поменяли значения
    retention: 6h

    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ## Прописываем нужный storageClass
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard-rwo
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    # volumeClaimTemplate:
    #   spec:
    #     storageClassName: gluster
    #     accessModes: ["ReadWriteOnce"]
    #     resources:
    #       requests:
    #         storage: 50Gi
    #     selector: {}

    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## Прописываем ресурсы
    resources:
      requests:
        memory: 300Mi
        cpu: "0.2"
      limits:
        memory: 300Mi
        cpu: "0.2"

    # requests:
    #   memory: 400Mi

    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
    ##
    externalUrl: https://alertmanager.kubernetes.basov.world

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    ## Гарантирует что не положит две реплики пода на одну ноду
    podAntiAffinity: "hard"

## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
##
grafana:
  enabled: true

  ## Timezone for the default dashboards
  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
  ##
  defaultDashboardsTimezone: "Europe/Moscow"
  
  adminPassword: prom-operator
  # Создаем секрет 
  # htpasswd -c auth admin
  # kubectl create secret generic admin-basic-auth --from-literal=admin-user=admin --from-file=admin-password=auth -n prometheus-oper
  # adminUser: admin
  # Authentication через секрет
  # admin:
  #   existingSecret: "admin-basic-auth" 
  #   userKey: admin-user
  #   passwordKey: admin-password

  ingress:
    ## If true, Grafana Ingress will be created
    ##
    enabled: true

    ## IngressClassName for Grafana Ingress.
    ## Should be provided if Ingress is enable.
    ##
    ingressClassName: nginx

    ## Annotations for Grafana Ingress
    ##
    annotations:
      kubernetes.io/tls-acme: "true"
      acme.cert-manager.io/http01-edit-in-place: "true"
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"

    ## Labels to be added to the Ingress
    ##
    labels: {}

    ## Hostnames.
    ## Must be provided if Ingress is enable.
    ##
    # hosts:
    #   - grafana.domain.com
    hosts:
      - grafana.kubernetes.basov.world

    ## Path for grafana ingress
    path: /

    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
      - secretName: grafana-general-tls
        hosts:
          - grafana.kubernetes.basov.world
    # - secretName: grafana-general-tls
    #   hosts:
    #   - grafana.example.com

  resources:
    requests:
      memory: 400Mi
      cpu: "0.3"
    limits:
      memory: 800Mi
      cpu: "0.3"

  serviceMonitor:
    # labels for the ServiceMonitor
    # Bug c версией? пропущен нужный label для мониторинга Grafana Metrics
    # prom-operator-grafana.prometheus-oper.svc.cluster.local/metrics
    # prom-operator это имя нашего релиза.
    labels:
      release: prom-operator


## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
  enabled: true
  serviceMonitor:
    ## Enable scraping /metrics/resource from kubelet's service
    ## This is disabled by default because container metrics are already exposed by cAdvisor
    ##
    resource: true
    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
    resourcePath: "/metrics/resource"
    # Настройки для мониторинга в Lens. ? Работает если выбираем Helm 14? Заработало на autodetect по непонятной причине
    metricRelabelings:
    - action: replace
      sourceLabels:
      - node
      targetLabel: instance

## Configuration for kube-state-metrics subchart
##
kube-state-metrics:

  resources:
    limits:
      cpu: 20m
      memory: 128Mi
    requests:
      cpu: 20m
      memory: 128Mi

prometheus-node-exporter:

  resources:
    requests:
      memory: 256Mi
      cpu: "0.1"
    limits:
      memory: 256Mi
      cpu: "0.1"


# Настройки для мониторинга в Lens. ? Работает если выбираем Helm 14? Заработало на autodetect по непонятной причине
  prometheus:
    monitor:
      metricRelabelings:
      - action: replace
        regex: (.*)
        replacement: $1
        sourceLabels:
        - __meta_kubernetes_pod_node_name
        targetLabel: kubernetes_node    
## Manages Prometheus and Alertmanager components
##
prometheusOperator:

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
    ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
    # Изменил значение
    failurePolicy: "Fail"

  ## Resource limits & requests
  ##
  resources:
    limits:
      cpu: 200m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 512Mi

  ## Prometheus-config-reloader
  ##
  prometheusConfigReloader:
    resources:
      requests:
        cpu: 50m
        memory: 25Mi
      limits:
        cpu: 50m
        memory: 25Mi

## Deploy a Prometheus instance
##
prometheus:
  enabled: true

  ingress:
    enabled: true

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    ingressClassName: nginx

    annotations:
      nginx.ingress.kubernetes.io/auth-type: 'basic'
      nginx.ingress.kubernetes.io/auth-secret: 'admin-basic-auth'
      nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
      kubernetes.io/tls-acme: "true"
      acme.cert-manager.io/http01-edit-in-place: "true"

    labels: {}

    ## Redirect ingress to an additional defined port on the service
    # servicePort: 8081

    ## Hostnames.
    ## Must be provided if Ingress is enabled.
    ##
    # hosts:
    #   - prometheus.domain.com
    hosts:
      - prometheus.kubernetes.basov.world

    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
    ##
    paths:
      - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific
    pathType: Prefix

    ## TLS configuration for Prometheus Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
      - secretName: prometheus-general-tls
        hosts:
          - prometheus.kubernetes.basov.world
      # - secretName: prometheus-general-tls
      #   hosts:
      #     - prometheus.example.com

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:

    ## If true, the Operator won't process any Prometheus configuration changes
    ##
    paused: false
    ## Maximum size of metrics
    ##
    retentionSize: "3000MB"

    ## Number of replicas of each shard to deploy for a Prometheus deployment.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ##
    replicas: 1

    ## Resource limits & requests
    ##
    resources:
      limits:
        cpu: "0.3"
        memory: 1Gi
      requests:
        cpu: "0.3"
        memory: 1Gi
        
    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard-rwo
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    #serviceMonitorSelectorNilUsesHelmValues: false

    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    ##
    #podMonitorSelectorNilUsesHelmValues: false

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: false

# Для GCP этот должен быть true вместо coreDns
## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: true


## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: false
## Component scraping kube proxy
##
kubeProxy:
  enabled: false
  service:
    enabled: true
    port: 10249
    targetPort: 10249
## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false
